{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d7661e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2fdf2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=open('names.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1008ab3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b657e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "#build voabulary of character and mapping to/from integrers\n",
    "chars=sorted(list(set(''.join(words))))\n",
    "stoi={s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.']=0\n",
    "itos={i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e048b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the logic of above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baf1f3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "67dd89aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n"
     ]
    }
   ],
   "source": [
    "#build the data set\n",
    "\n",
    "block_size=3 #context lenght: how many inputs/characters we take to predict next char\n",
    "X,Y=[],[] #X are the inputs to NN, Y is the label for each input inside X\n",
    "\n",
    "for w in words[:1]: #gives the name/word ,first 5 so in loop it is emma\n",
    "    print(w)\n",
    "    context=block_size*[0]   #1st loop : [0,0,0]\n",
    "    for ch in w + '.':       #adds . to the name and gives the  letter here e\n",
    "        ix=stoi[ch]          #gives corresponding int value of e and stores in ix\n",
    "        X.append(context)    #adds [0,0,0] to X,list inside list\n",
    "        Y.append(ix)         #add ix to Y, here for e\n",
    "        print(''.join(itos[i] for i in context),'--->',itos[ix])\n",
    "        context=context[1:]+ [ix] #crop and append: before starting for m (next letter in emma), removes first 0 from context and adds the value of e as third elemnt in context \n",
    "        \n",
    "X=torch.tensor(X)\n",
    "Y=torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d47e578e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3bfa675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3368f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so when ... is input e is the label, ie 0,0,0 and 5 respectively, \n",
    "#if change block size to x we taking x inputs to predict x+1th char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "91d26f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "#build the data set\n",
    "\n",
    "block_size=3 #context lenght: how many inputs/characters we take to predict next char\n",
    "X,Y=[],[] #X are the inputs to NN, Y is the label for each input inside X\n",
    "\n",
    "for w in words[:5]: #gives the name/word ,first 5 so in loop it is emma\n",
    "    print(w)\n",
    "    context=block_size*[0]   #1st loop : [0,0,0]\n",
    "    for ch in w + '.':       #adds . to the name and gives the  letter here e\n",
    "        ix=stoi[ch]          #gives corresponding int value of e and stores in ix\n",
    "        X.append(context)    #adds [0,0,0] to X,list inside list\n",
    "        Y.append(ix)         #add ix to Y, here for e\n",
    "        print(''.join(itos[i] for i in context),'--->',itos[ix])\n",
    "        context=context[1:]+ [ix] #crop and append: before starting for m (next letter in emma), removes first 0 from context and adds the value of e as third elemnt in context \n",
    "        \n",
    "X=torch.tensor(X)\n",
    "Y=torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2daaaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a NN which takes X and predicts Y\n",
    "# 27 possible char and embed in 2 dimentional space: Lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "155e2059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6207,  0.0059],\n",
      "        [-1.0589,  0.8934],\n",
      "        [ 1.1340, -0.4418],\n",
      "        [-1.2516,  0.6136],\n",
      "        [-1.8992,  1.0459],\n",
      "        [ 0.2416,  0.3255],\n",
      "        [-0.2781,  1.7812],\n",
      "        [ 0.4611, -0.2713],\n",
      "        [-0.3733,  1.3076],\n",
      "        [-0.7259,  0.2654],\n",
      "        [ 0.1460,  0.7682],\n",
      "        [ 0.7569,  0.0990],\n",
      "        [-0.7252, -1.3505],\n",
      "        [-0.4955,  0.8390],\n",
      "        [ 0.3995,  0.2891],\n",
      "        [ 0.2624,  1.9903],\n",
      "        [ 0.5670,  0.7334],\n",
      "        [-0.0364,  0.4822],\n",
      "        [-0.0532,  1.1127],\n",
      "        [ 0.5514, -0.4101],\n",
      "        [ 1.0611, -0.1791],\n",
      "        [ 0.7898, -0.6618],\n",
      "        [-0.4426,  0.9759],\n",
      "        [-1.2491,  0.6367],\n",
      "        [-1.1948, -0.8110],\n",
      "        [-0.1587,  0.6298],\n",
      "        [-0.4159,  0.0736]])\n"
     ]
    }
   ],
   "source": [
    "C=torch.randn((27,2))\n",
    "print(C)# weights initialized randomly, matrix c of embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c9efa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d54898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedd single indi int say 5\n",
    "#two ways to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "250fb4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2416, 0.3255])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ee76686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2: OHE\n",
    "F.one_hot(torch.tensor(5),num_classes=27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d31b63bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2416, 0.3255])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5),num_classes=27).float() @ C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''[0000100---0]1x27 *  _   _   =[_,_]1x2\n",
    "                    |    | \n",
    "                    |    |\n",
    "                    |_  _|27x2\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "507a058d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,X.dtype,Y.shape,Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1adb3607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same result\n",
    "# we will use former the indexing bcx faster: index int and use embedding int\n",
    "# embedding a single int like 5 is easy : C[5] but we have to do this for X which is 32,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c24080d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2416,  0.3255],\n",
       "        [-0.2781,  1.7812],\n",
       "        [ 0.4611, -0.2713]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[[5,6,7]] #index with list:allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9954714c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2416,  0.3255],\n",
       "        [-0.2781,  1.7812],\n",
       "        [ 0.4611, -0.2713],\n",
       "        [ 0.4611, -0.2713],\n",
       "        [ 0.4611, -0.2713],\n",
       "        [ 0.4611, -0.2713]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[torch.tensor([5,6,7,7,7,7])] #indexing with tensor and repetition allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4fe435b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059]],\n",
       "\n",
       "        [[-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [ 0.2416,  0.3255]],\n",
       "\n",
       "        [[-0.6207,  0.0059],\n",
       "         [ 0.2416,  0.3255],\n",
       "         [-0.4955,  0.8390]],\n",
       "\n",
       "        [[ 0.2416,  0.3255],\n",
       "         [-0.4955,  0.8390],\n",
       "         [-0.4955,  0.8390]],\n",
       "\n",
       "        [[-0.4955,  0.8390],\n",
       "         [-0.4955,  0.8390],\n",
       "         [-1.0589,  0.8934]],\n",
       "\n",
       "        [[-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059]],\n",
       "\n",
       "        [[-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [ 0.2624,  1.9903]],\n",
       "\n",
       "        [[-0.6207,  0.0059],\n",
       "         [ 0.2624,  1.9903],\n",
       "         [-0.7252, -1.3505]],\n",
       "\n",
       "        [[ 0.2624,  1.9903],\n",
       "         [-0.7252, -1.3505],\n",
       "         [-0.7259,  0.2654]],\n",
       "\n",
       "        [[-0.7252, -1.3505],\n",
       "         [-0.7259,  0.2654],\n",
       "         [-0.4426,  0.9759]],\n",
       "\n",
       "        [[-0.7259,  0.2654],\n",
       "         [-0.4426,  0.9759],\n",
       "         [-0.7259,  0.2654]],\n",
       "\n",
       "        [[-0.4426,  0.9759],\n",
       "         [-0.7259,  0.2654],\n",
       "         [-1.0589,  0.8934]],\n",
       "\n",
       "        [[-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059]],\n",
       "\n",
       "        [[-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-1.0589,  0.8934]],\n",
       "\n",
       "        [[-0.6207,  0.0059],\n",
       "         [-1.0589,  0.8934],\n",
       "         [-0.4426,  0.9759]],\n",
       "\n",
       "        [[-1.0589,  0.8934],\n",
       "         [-0.4426,  0.9759],\n",
       "         [-1.0589,  0.8934]],\n",
       "\n",
       "        [[-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059]],\n",
       "\n",
       "        [[-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.7259,  0.2654]],\n",
       "\n",
       "        [[-0.6207,  0.0059],\n",
       "         [-0.7259,  0.2654],\n",
       "         [ 0.5514, -0.4101]],\n",
       "\n",
       "        [[-0.7259,  0.2654],\n",
       "         [ 0.5514, -0.4101],\n",
       "         [-1.0589,  0.8934]],\n",
       "\n",
       "        [[ 0.5514, -0.4101],\n",
       "         [-1.0589,  0.8934],\n",
       "         [ 1.1340, -0.4418]],\n",
       "\n",
       "        [[-1.0589,  0.8934],\n",
       "         [ 1.1340, -0.4418],\n",
       "         [ 0.2416,  0.3255]],\n",
       "\n",
       "        [[ 1.1340, -0.4418],\n",
       "         [ 0.2416,  0.3255],\n",
       "         [-0.7252, -1.3505]],\n",
       "\n",
       "        [[ 0.2416,  0.3255],\n",
       "         [-0.7252, -1.3505],\n",
       "         [-0.7252, -1.3505]],\n",
       "\n",
       "        [[-0.7252, -1.3505],\n",
       "         [-0.7252, -1.3505],\n",
       "         [-1.0589,  0.8934]],\n",
       "\n",
       "        [[-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059]],\n",
       "\n",
       "        [[-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [ 0.5514, -0.4101]],\n",
       "\n",
       "        [[-0.6207,  0.0059],\n",
       "         [ 0.5514, -0.4101],\n",
       "         [ 0.2624,  1.9903]],\n",
       "\n",
       "        [[ 0.5514, -0.4101],\n",
       "         [ 0.2624,  1.9903],\n",
       "         [ 0.5670,  0.7334]],\n",
       "\n",
       "        [[ 0.2624,  1.9903],\n",
       "         [ 0.5670,  0.7334],\n",
       "         [-0.3733,  1.3076]],\n",
       "\n",
       "        [[ 0.5670,  0.7334],\n",
       "         [-0.3733,  1.3076],\n",
       "         [-0.7259,  0.2654]],\n",
       "\n",
       "        [[-0.3733,  1.3076],\n",
       "         [-0.7259,  0.2654],\n",
       "         [-1.0589,  0.8934]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X]                  #indexing with multidimentional tensor allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "877eb545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape #here 2 is the embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b786ef50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[13,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "476e8a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7be99dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[13,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0404d028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0589,  0.8934])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X[13,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aff98d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0589,  0.8934])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d1450f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb780024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X is the array with inputs: here  32X3 , X.append(context), context=[0]*3 and so on, C is a random array 27x2\n",
    "#so C[X] is 32x3x3 which means that X is used as index to get values fro C, so if we ask for C[X][13,2] we go to X\n",
    "    #and see\n",
    "# X[13,2] is the element of 13th row, 2nd col (remember python starts 0) which is 1\n",
    "#so C[X[13,2]]=C[1] is [-1.0589,  0.8934] which is nothing but 1st row of C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a6ce0ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8934)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rough work to understand this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f4a723d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.6207,  0.0059],\n",
       "         [-1.0589,  0.8934],\n",
       "         [ 1.1340, -0.4418],\n",
       "         [-1.2516,  0.6136],\n",
       "         [-1.8992,  1.0459],\n",
       "         [ 0.2416,  0.3255],\n",
       "         [-0.2781,  1.7812],\n",
       "         [ 0.4611, -0.2713],\n",
       "         [-0.3733,  1.3076],\n",
       "         [-0.7259,  0.2654],\n",
       "         [ 0.1460,  0.7682],\n",
       "         [ 0.7569,  0.0990],\n",
       "         [-0.7252, -1.3505],\n",
       "         [-0.4955,  0.8390],\n",
       "         [ 0.3995,  0.2891],\n",
       "         [ 0.2624,  1.9903],\n",
       "         [ 0.5670,  0.7334],\n",
       "         [-0.0364,  0.4822],\n",
       "         [-0.0532,  1.1127],\n",
       "         [ 0.5514, -0.4101],\n",
       "         [ 1.0611, -0.1791],\n",
       "         [ 0.7898, -0.6618],\n",
       "         [-0.4426,  0.9759],\n",
       "         [-1.2491,  0.6367],\n",
       "         [-1.1948, -0.8110],\n",
       "         [-0.1587,  0.6298],\n",
       "         [-0.4159,  0.0736]]),\n",
       " tensor([[ 0,  0,  0],\n",
       "         [ 0,  0,  5],\n",
       "         [ 0,  5, 13],\n",
       "         [ 5, 13, 13],\n",
       "         [13, 13,  1],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0, 15],\n",
       "         [ 0, 15, 12],\n",
       "         [15, 12,  9],\n",
       "         [12,  9, 22],\n",
       "         [ 9, 22,  9],\n",
       "         [22,  9,  1],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  1],\n",
       "         [ 0,  1, 22],\n",
       "         [ 1, 22,  1],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  9],\n",
       "         [ 0,  9, 19],\n",
       "         [ 9, 19,  1],\n",
       "         [19,  1,  2],\n",
       "         [ 1,  2,  5],\n",
       "         [ 2,  5, 12],\n",
       "         [ 5, 12, 12],\n",
       "         [12, 12,  1],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0, 19],\n",
       "         [ 0, 19, 15],\n",
       "         [19, 15, 16],\n",
       "         [15, 16,  8],\n",
       "         [16,  8,  9],\n",
       "         [ 8,  9,  1]]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0084853d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 5]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z=torch.tensor([[0,0,0],\n",
    "  [0,0,5]])\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d94bdd97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059]],\n",
       "\n",
       "        [[-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [ 0.2416,  0.3255]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[Z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "892e65b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[Z].shape #rough done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a55601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3690c7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb=C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1151d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now contruct the hidden layer\n",
    "#no of inputs to this layer 3*2=6 bcz we have 2 dim embedings and 3 (3 char we talked abiut as inputs) of them\n",
    "\n",
    "\n",
    "W1=torch.randn((6,100)) #no of neurons is upto us here 100\n",
    "b1=torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5976b9b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#what we want is :\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m emb \u001b[38;5;241m@\u001b[39m W1 \u001b[38;5;241m+\u001b[39m b1\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)"
     ]
    }
   ],
   "source": [
    "#what we want is :\n",
    "emb @ W1 + b1\n",
    "#but the problem is that these emb ar stacked up in the dim of the input tensor: [32, 3, 2] ie [32, 3, 2]!*[6,100]\n",
    "# somehow concatenate the inputs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c155e1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# many ways to implement this in torch: use cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "24f53f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059]],\n",
       "\n",
       "        [[-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [ 0.2416,  0.3255]],\n",
       "\n",
       "        [[-0.6207,  0.0059],\n",
       "         [ 0.2416,  0.3255],\n",
       "         [-0.4955,  0.8390]],\n",
       "\n",
       "        [[ 0.2416,  0.3255],\n",
       "         [-0.4955,  0.8390],\n",
       "         [-0.4955,  0.8390]],\n",
       "\n",
       "        [[-0.4955,  0.8390],\n",
       "         [-0.4955,  0.8390],\n",
       "         [-1.0589,  0.8934]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b01bcd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6207,  0.0059],\n",
       "        [-0.6207,  0.0059],\n",
       "        [-0.6207,  0.0059],\n",
       "        [ 0.2416,  0.3255],\n",
       "        [-0.4955,  0.8390]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[:,0,:][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "771a0066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6207,  0.0059],\n",
       "        [-0.6207,  0.0059],\n",
       "        [ 0.2416,  0.3255],\n",
       "        [-0.4955,  0.8390],\n",
       "        [-0.4955,  0.8390]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[:,1,:][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bd1f3ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6207,  0.0059],\n",
       "        [ 0.2416,  0.3255],\n",
       "        [-0.4955,  0.8390],\n",
       "        [-0.4955,  0.8390],\n",
       "        [-1.0589,  0.8934]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[:,2,:][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2948c62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[:,0,:].shape #plucks out 32x2 embedding of just the first char/letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "03d9b72c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([emb[:,0,:],emb[:,1,:],emb[:,2,:]],1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eb2bd0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6207,  0.0059, -0.6207,  0.0059, -0.6207,  0.0059],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059,  0.2416,  0.3255],\n",
       "        [-0.6207,  0.0059,  0.2416,  0.3255, -0.4955,  0.8390],\n",
       "        [ 0.2416,  0.3255, -0.4955,  0.8390, -0.4955,  0.8390],\n",
       "        [-0.4955,  0.8390, -0.4955,  0.8390, -1.0589,  0.8934],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059, -0.6207,  0.0059],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059,  0.2624,  1.9903],\n",
       "        [-0.6207,  0.0059,  0.2624,  1.9903, -0.7252, -1.3505],\n",
       "        [ 0.2624,  1.9903, -0.7252, -1.3505, -0.7259,  0.2654],\n",
       "        [-0.7252, -1.3505, -0.7259,  0.2654, -0.4426,  0.9759],\n",
       "        [-0.7259,  0.2654, -0.4426,  0.9759, -0.7259,  0.2654],\n",
       "        [-0.4426,  0.9759, -0.7259,  0.2654, -1.0589,  0.8934],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059, -0.6207,  0.0059],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059, -1.0589,  0.8934],\n",
       "        [-0.6207,  0.0059, -1.0589,  0.8934, -0.4426,  0.9759],\n",
       "        [-1.0589,  0.8934, -0.4426,  0.9759, -1.0589,  0.8934],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059, -0.6207,  0.0059],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059, -0.7259,  0.2654],\n",
       "        [-0.6207,  0.0059, -0.7259,  0.2654,  0.5514, -0.4101],\n",
       "        [-0.7259,  0.2654,  0.5514, -0.4101, -1.0589,  0.8934],\n",
       "        [ 0.5514, -0.4101, -1.0589,  0.8934,  1.1340, -0.4418],\n",
       "        [-1.0589,  0.8934,  1.1340, -0.4418,  0.2416,  0.3255],\n",
       "        [ 1.1340, -0.4418,  0.2416,  0.3255, -0.7252, -1.3505],\n",
       "        [ 0.2416,  0.3255, -0.7252, -1.3505, -0.7252, -1.3505],\n",
       "        [-0.7252, -1.3505, -0.7252, -1.3505, -1.0589,  0.8934],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059, -0.6207,  0.0059],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059,  0.5514, -0.4101],\n",
       "        [-0.6207,  0.0059,  0.5514, -0.4101,  0.2624,  1.9903],\n",
       "        [ 0.5514, -0.4101,  0.2624,  1.9903,  0.5670,  0.7334],\n",
       "        [ 0.2624,  1.9903,  0.5670,  0.7334, -0.3733,  1.3076],\n",
       "        [ 0.5670,  0.7334, -0.3733,  1.3076, -0.7259,  0.2654],\n",
       "        [-0.3733,  1.3076, -0.7259,  0.2654, -1.0589,  0.8934]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([emb[:,0,:],emb[:,1,:],emb[:,2,:]],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2b68b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for generalizing block size: fn unbind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b9bdde97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [ 0.2416,  0.3255],\n",
       "         [-0.4955,  0.8390],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [ 0.2624,  1.9903],\n",
       "         [-0.7252, -1.3505],\n",
       "         [-0.7259,  0.2654],\n",
       "         [-0.4426,  0.9759],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-1.0589,  0.8934],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.7259,  0.2654],\n",
       "         [ 0.5514, -0.4101],\n",
       "         [-1.0589,  0.8934],\n",
       "         [ 1.1340, -0.4418],\n",
       "         [ 0.2416,  0.3255],\n",
       "         [-0.7252, -1.3505],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [ 0.5514, -0.4101],\n",
       "         [ 0.2624,  1.9903],\n",
       "         [ 0.5670,  0.7334],\n",
       "         [-0.3733,  1.3076]]),\n",
       " tensor([[-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [ 0.2416,  0.3255],\n",
       "         [-0.4955,  0.8390],\n",
       "         [-0.4955,  0.8390],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [ 0.2624,  1.9903],\n",
       "         [-0.7252, -1.3505],\n",
       "         [-0.7259,  0.2654],\n",
       "         [-0.4426,  0.9759],\n",
       "         [-0.7259,  0.2654],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-1.0589,  0.8934],\n",
       "         [-0.4426,  0.9759],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.7259,  0.2654],\n",
       "         [ 0.5514, -0.4101],\n",
       "         [-1.0589,  0.8934],\n",
       "         [ 1.1340, -0.4418],\n",
       "         [ 0.2416,  0.3255],\n",
       "         [-0.7252, -1.3505],\n",
       "         [-0.7252, -1.3505],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.6207,  0.0059],\n",
       "         [ 0.5514, -0.4101],\n",
       "         [ 0.2624,  1.9903],\n",
       "         [ 0.5670,  0.7334],\n",
       "         [-0.3733,  1.3076],\n",
       "         [-0.7259,  0.2654]]),\n",
       " tensor([[-0.6207,  0.0059],\n",
       "         [ 0.2416,  0.3255],\n",
       "         [-0.4955,  0.8390],\n",
       "         [-0.4955,  0.8390],\n",
       "         [-1.0589,  0.8934],\n",
       "         [-0.6207,  0.0059],\n",
       "         [ 0.2624,  1.9903],\n",
       "         [-0.7252, -1.3505],\n",
       "         [-0.7259,  0.2654],\n",
       "         [-0.4426,  0.9759],\n",
       "         [-0.7259,  0.2654],\n",
       "         [-1.0589,  0.8934],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-1.0589,  0.8934],\n",
       "         [-0.4426,  0.9759],\n",
       "         [-1.0589,  0.8934],\n",
       "         [-0.6207,  0.0059],\n",
       "         [-0.7259,  0.2654],\n",
       "         [ 0.5514, -0.4101],\n",
       "         [-1.0589,  0.8934],\n",
       "         [ 1.1340, -0.4418],\n",
       "         [ 0.2416,  0.3255],\n",
       "         [-0.7252, -1.3505],\n",
       "         [-0.7252, -1.3505],\n",
       "         [-1.0589,  0.8934],\n",
       "         [-0.6207,  0.0059],\n",
       "         [ 0.5514, -0.4101],\n",
       "         [ 0.2624,  1.9903],\n",
       "         [ 0.5670,  0.7334],\n",
       "         [-0.3733,  1.3076],\n",
       "         [-0.7259,  0.2654],\n",
       "         [-1.0589,  0.8934]]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unbind(emb,1) #list of tensors equal to [emb[:,0,:],emb[:,1,:],emb[:,2,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d896e747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch.unbind(emb,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "226bd7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6207,  0.0059, -0.6207,  0.0059, -0.6207,  0.0059],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059,  0.2416,  0.3255],\n",
       "        [-0.6207,  0.0059,  0.2416,  0.3255, -0.4955,  0.8390],\n",
       "        [ 0.2416,  0.3255, -0.4955,  0.8390, -0.4955,  0.8390],\n",
       "        [-0.4955,  0.8390, -0.4955,  0.8390, -1.0589,  0.8934],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059, -0.6207,  0.0059],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059,  0.2624,  1.9903],\n",
       "        [-0.6207,  0.0059,  0.2624,  1.9903, -0.7252, -1.3505],\n",
       "        [ 0.2624,  1.9903, -0.7252, -1.3505, -0.7259,  0.2654],\n",
       "        [-0.7252, -1.3505, -0.7259,  0.2654, -0.4426,  0.9759],\n",
       "        [-0.7259,  0.2654, -0.4426,  0.9759, -0.7259,  0.2654],\n",
       "        [-0.4426,  0.9759, -0.7259,  0.2654, -1.0589,  0.8934],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059, -0.6207,  0.0059],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059, -1.0589,  0.8934],\n",
       "        [-0.6207,  0.0059, -1.0589,  0.8934, -0.4426,  0.9759],\n",
       "        [-1.0589,  0.8934, -0.4426,  0.9759, -1.0589,  0.8934],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059, -0.6207,  0.0059],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059, -0.7259,  0.2654],\n",
       "        [-0.6207,  0.0059, -0.7259,  0.2654,  0.5514, -0.4101],\n",
       "        [-0.7259,  0.2654,  0.5514, -0.4101, -1.0589,  0.8934],\n",
       "        [ 0.5514, -0.4101, -1.0589,  0.8934,  1.1340, -0.4418],\n",
       "        [-1.0589,  0.8934,  1.1340, -0.4418,  0.2416,  0.3255],\n",
       "        [ 1.1340, -0.4418,  0.2416,  0.3255, -0.7252, -1.3505],\n",
       "        [ 0.2416,  0.3255, -0.7252, -1.3505, -0.7252, -1.3505],\n",
       "        [-0.7252, -1.3505, -0.7252, -1.3505, -1.0589,  0.8934],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059, -0.6207,  0.0059],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059,  0.5514, -0.4101],\n",
       "        [-0.6207,  0.0059,  0.5514, -0.4101,  0.2624,  1.9903],\n",
       "        [ 0.5514, -0.4101,  0.2624,  1.9903,  0.5670,  0.7334],\n",
       "        [ 0.2624,  1.9903,  0.5670,  0.7334, -0.3733,  1.3076],\n",
       "        [ 0.5670,  0.7334, -0.3733,  1.3076, -0.7259,  0.2654],\n",
       "        [-0.3733,  1.3076, -0.7259,  0.2654, -1.0589,  0.8934]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(torch.unbind(emb,1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "43d7b541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(torch.unbind(emb,1),1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c3c9fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#even more efficient way of doing this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9a0b8f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.arange(18)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2f7a907d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#represent as diff sized n dim tensor\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d292f268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(3,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6b9b9bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "        [ 9, 10, 11, 12, 13, 14, 15, 16, 17]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(2,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cf9383ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_2416\\2019216251.py:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  a.storage()# all no as 1 dim vector as stored in memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       " 17\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#above is possible bcz of\n",
    "a.storage()# all no as 1 dim vector as stored in memory\n",
    "#check out blog by pytorch internal by eric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "760f27d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6207,  0.0059, -0.6207,  0.0059, -0.6207,  0.0059],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059,  0.2416,  0.3255],\n",
       "        [-0.6207,  0.0059,  0.2416,  0.3255, -0.4955,  0.8390],\n",
       "        [ 0.2416,  0.3255, -0.4955,  0.8390, -0.4955,  0.8390],\n",
       "        [-0.4955,  0.8390, -0.4955,  0.8390, -1.0589,  0.8934],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059, -0.6207,  0.0059],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059,  0.2624,  1.9903],\n",
       "        [-0.6207,  0.0059,  0.2624,  1.9903, -0.7252, -1.3505],\n",
       "        [ 0.2624,  1.9903, -0.7252, -1.3505, -0.7259,  0.2654],\n",
       "        [-0.7252, -1.3505, -0.7259,  0.2654, -0.4426,  0.9759],\n",
       "        [-0.7259,  0.2654, -0.4426,  0.9759, -0.7259,  0.2654],\n",
       "        [-0.4426,  0.9759, -0.7259,  0.2654, -1.0589,  0.8934],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059, -0.6207,  0.0059],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059, -1.0589,  0.8934],\n",
       "        [-0.6207,  0.0059, -1.0589,  0.8934, -0.4426,  0.9759],\n",
       "        [-1.0589,  0.8934, -0.4426,  0.9759, -1.0589,  0.8934],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059, -0.6207,  0.0059],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059, -0.7259,  0.2654],\n",
       "        [-0.6207,  0.0059, -0.7259,  0.2654,  0.5514, -0.4101],\n",
       "        [-0.7259,  0.2654,  0.5514, -0.4101, -1.0589,  0.8934],\n",
       "        [ 0.5514, -0.4101, -1.0589,  0.8934,  1.1340, -0.4418],\n",
       "        [-1.0589,  0.8934,  1.1340, -0.4418,  0.2416,  0.3255],\n",
       "        [ 1.1340, -0.4418,  0.2416,  0.3255, -0.7252, -1.3505],\n",
       "        [ 0.2416,  0.3255, -0.7252, -1.3505, -0.7252, -1.3505],\n",
       "        [-0.7252, -1.3505, -0.7252, -1.3505, -1.0589,  0.8934],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059, -0.6207,  0.0059],\n",
       "        [-0.6207,  0.0059, -0.6207,  0.0059,  0.5514, -0.4101],\n",
       "        [-0.6207,  0.0059,  0.5514, -0.4101,  0.2624,  1.9903],\n",
       "        [ 0.5514, -0.4101,  0.2624,  1.9903,  0.5670,  0.7334],\n",
       "        [ 0.2624,  1.9903,  0.5670,  0.7334, -0.3733,  1.3076],\n",
       "        [ 0.5670,  0.7334, -0.3733,  1.3076, -0.7259,  0.2654],\n",
       "        [-0.3733,  1.3076, -0.7259,  0.2654, -1.0589,  0.8934]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939ddee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d0273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753edbd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f7fa4412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8543,  0.2955,  1.6099,  ...,  3.1177, -1.2067,  2.8979],\n",
       "        [-0.6935, -1.3648,  1.7065,  ...,  2.6310, -0.3412,  2.0098],\n",
       "        [ 0.7945, -1.3494, -0.6717,  ...,  3.4557,  0.4316,  1.1501],\n",
       "        ...,\n",
       "        [-3.6735, -0.3060, -2.3980,  ...,  3.1972,  1.1600,  1.7365],\n",
       "        [-3.6674,  2.8005, -0.2152,  ...,  1.8019, -0.1070,  3.2112],\n",
       "        [-3.9465, -0.4432,  0.8269,  ...,  5.5574, -0.4634,  3.2628]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emb.view(emb.shape([0])) @ W1 + b1 or use view\n",
    "emb.view(-1,6) @ W1 + b1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e4d0a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h=torch.tanh(emb.view(-1,6) @ W1 + b1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "438167dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6933,  0.2872,  0.9231,  ...,  0.9961, -0.8357,  0.9939],\n",
       "        [-0.6002, -0.8775,  0.9362,  ...,  0.9897, -0.3286,  0.9647],\n",
       "        [ 0.6610, -0.8739, -0.5861,  ...,  0.9980,  0.4067,  0.8178],\n",
       "        ...,\n",
       "        [-0.9987, -0.2968, -0.9836,  ...,  0.9967,  0.8210,  0.9398],\n",
       "        [-0.9987,  0.9926, -0.2119,  ...,  0.9470, -0.1066,  0.9968],\n",
       "        [-0.9993, -0.4163,  0.6788,  ...,  1.0000, -0.4328,  0.9971]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "62219641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape # this is the hidden layer of activation for our 32 examples and 100 neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "40e0f067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5040ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32,100\n",
    "# 1 ,100 row vector , 1 fake created \n",
    "# same bias vector will be added to all the rows of h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613beb35",
   "metadata": {},
   "source": [
    "# final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d468ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2=torch.randn((100,27)) #input 100 from previous layer and out is obv 27\n",
    "b2=torch.randn(27) #so biases will also be 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6bd00874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits=h @ W2 +b2   #logits are outputs of this nn, remember h is out put of last layer shape:32x100\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e9014be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=logits.exp() #fake counts from exp, normalise to prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e852915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob=counts/counts.sum(1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a42a3054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4e0042d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index into rows of prob and each row we would like to pluck out the probabilty assigned to the correct char as given below by Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "518fb86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "245692c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.1744e-02, 1.5757e-06, 1.2412e-05, 6.0616e-10, 3.3164e-09, 4.0265e-10,\n",
       "        3.1021e-05, 1.3066e-06, 1.3981e-08, 1.0000e+00, 2.4705e-10, 2.0856e-08,\n",
       "        2.0580e-10, 2.1942e-05, 1.0986e-09, 3.6561e-09, 4.5328e-01, 3.7496e-13,\n",
       "        5.4310e-07, 1.9225e-11, 1.8135e-11, 1.7640e-12, 7.1803e-06, 1.0509e-04,\n",
       "        1.1736e-11, 7.1027e-13, 2.3467e-10, 8.0314e-08, 1.3618e-02, 1.3769e-11,\n",
       "        3.9510e-11, 6.5968e-07])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[torch.arange(32),Y]\n",
    "#this gives currant probabilties as assigned by this(torch.arange(32)) neural network with this (W2) \n",
    "# setting of weights to the correct char in the sequence\n",
    "\n",
    "#some are okay like  8.1744e-02 which is .08 but some are too small like 2.4705e-10= 0.000000000245 \n",
    "# but we havent trained neural net yet\n",
    "\n",
    "# ideally all these no should be very close to 1 for good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360565c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "603577a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#actual layer from Y\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "80830897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.5812)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#negative log likelihood loss\n",
    "loss=-prob[torch.arange(32)].log().mean()\n",
    "loss\n",
    "#this is the loss we want to minimise for the network to predict properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2c83da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------more comprehensive----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c8436bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape #dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b7ac0368",
   "metadata": {},
   "outputs": [],
   "source": [
    "g=torch.Generator().manual_seed(2147483647) #reproducible\n",
    "C=torch.randn((27,2), generator=g)\n",
    "W1=torch.randn((6,100), generator=g)\n",
    "b1=torch.randn(100, generator=g)\n",
    "W2=torch.randn((100,27), generator=g)\n",
    "b2=torch.randn(27, generator=g)\n",
    "parameters=[C,W1,b1,W2,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "35a7fc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5ed4ac10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) #no of parameters in total\n",
    "# 27*2 + 6*100 +100 + 100*27 + 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6fe57683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb=C[X]\n",
    "h=torch.tanh(emb.view(-1,6) @ W1 +b1) #32x100\n",
    "logits=h @ W2 +b2 #32x27\n",
    "counts= logits.exp()\n",
    "prob=counts/counts.sum(1,keepdims=True)\n",
    "loss=-prob[torch.arange(32),Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a465e5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we should use pre build fn for loss\n",
    "\n",
    "emb=C[X]\n",
    "h=torch.tanh(emb.view(-1,6) @ W1 +b1) #32x100\n",
    "logits=h @ W2 +b2 #32x27\n",
    "loss=F.cross_entropy(logits,Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "83a2311b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.0466e-04, 3.3281e-04, 6.6846e-03, 9.9208e-01])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reasons as follows\n",
    "#1. no need to create new tensors and save memory through fusing kernel: counts,prob: efficient forward pass\n",
    "#2. Backward pass can be made more efficient\n",
    "#3. numerically well behaveed, eg:\n",
    "logits1=torch.tensor([-2,-3,0,5])\n",
    "counts1= logits1.exp()\n",
    "prob1=counts1/counts1.sum()\n",
    "# loss1=-prob[torch.arange(32),Y].log().mean()\n",
    "# loss1\n",
    "prob1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4ace89e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.6052e-45, 1.4013e-45, 3.7835e-44, 1.0000e+00])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#but if very positive logits, we get nan: run out of range in floating point no, can do -100  and offset so basically logits -100\n",
    "logits1=torch.tensor([-2,-3,0,100])# -100\n",
    "counts1= logits1.exp()\n",
    "prob1=counts1/counts1.sum()\n",
    "# loss1=-prob[torch.arange(32),Y].log().mean()\n",
    "# loss1\n",
    "prob1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6659ab6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d04c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c500a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a49cf009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.251748651266098\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):   \n",
    "    emb=C[X]\n",
    "    h=torch.tanh(emb.view(-1,6) @ W1 +b1) #32x100\n",
    "    logits=h @ W2 +b2 #32x27\n",
    "    loss=F.cross_entropy(logits,Y)\n",
    "#     print(loss.item())\n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad=None            #set gradients to 0\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    #upgarde\n",
    "    for p in parameters:\n",
    "        p.data+= -0.1*p.grad\n",
    "        \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "4e1b1974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#till now we only had 32 example from first 5 words (recall X), so easy to make this nn fit for 3k parameter for 32 examples\n",
    "#we are not able to achieve exactly 0 , reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "8484aa41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([15.4534, 21.7150, 22.6501, 23.3305, 19.6756, 15.4534, 18.9005, 16.4614,\n",
       "        18.1132, 21.7582, 19.0179, 23.9170, 15.4534, 20.6877, 20.4342, 23.3904,\n",
       "        15.4534, 19.7418, 18.9652, 20.6242, 21.0806, 19.2845, 13.8394, 13.6817,\n",
       "        17.7964, 15.4534, 18.8324, 19.6703, 15.0163, 18.6938, 22.7255, 20.1244],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([ 9, 13, 13,  1,  0,  9, 12,  9, 22,  9,  1,  0,  9, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0,  9, 15, 16,  8,  9,  1,  0]))"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pytorch records both the actual values : 15.4534, 21.715... on the maximum no and also indices whcih are very close to labels Y\n",
    "#9 vs 5: for ... possible outputs are e,o,a,i,a\n",
    "#for unique input output better pred\n",
    "logits.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8989c2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "665d8412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for full dataset\n",
    "\n",
    "\n",
    "#build the data set\n",
    "\n",
    "block_size=3 #context lenght: how many inputs/characters we take to predict next char\n",
    "X,Y=[],[] #X are the inputs to NN, Y is the label for each input inside X\n",
    "\n",
    "for w in words: #gives the name/word ,first 5 so in loop it is emma\n",
    "#     print(w)\n",
    "    context=block_size*[0]   #1st loop : [0,0,0]\n",
    "    for ch in w + '.':       #adds . to the name and gives the  letter here e\n",
    "        ix=stoi[ch]          #gives corresponding int value of e and stores in ix\n",
    "        X.append(context)    #adds [0,0,0] to X,list inside list\n",
    "        Y.append(ix)         #add ix to Y, here for e\n",
    "#         print(''.join(itos[i] for i in context),'--->',itos[ix])\n",
    "        context=context[1:]+ [ix] #crop and append: before starting for m (next letter in emma), removes first 0 from context and adds the value of e as third elemnt in context \n",
    "        \n",
    "X=torch.tensor(X)\n",
    "Y=torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "20882f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype,Y.shape,Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "88d2a255",
   "metadata": {},
   "outputs": [],
   "source": [
    "g=torch.Generator().manual_seed(2147483647) #reproducible\n",
    "C=torch.randn((27,2), generator=g)\n",
    "W1=torch.randn((6,100), generator=g)\n",
    "b1=torch.randn(100, generator=g)\n",
    "W2=torch.randn((100,27), generator=g)\n",
    "b2=torch.randn(27, generator=g)\n",
    "parameters=[C,W1,b1,W2,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "415ce3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "b2a47978",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "1e70184f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.505229949951172\n",
      "17.084491729736328\n",
      "15.776531219482422\n",
      "14.833340644836426\n",
      "14.002603530883789\n",
      "13.253260612487793\n",
      "12.579917907714844\n",
      "11.983101844787598\n",
      "11.470492362976074\n",
      "11.05185604095459\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10): \n",
    "    \n",
    "    #forwardpass\n",
    "    emb=C[X] #(228146,3,2)\n",
    "    h=torch.tanh(emb.view(-1,6) @ W1 +b1) #32x100\n",
    "    logits=h @ W2 +b2 #32x27\n",
    "    loss=F.cross_entropy(logits,Y)\n",
    "    print(loss.item())\n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad=None            #set gradients to 0\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    #upgarde\n",
    "    for p in parameters:\n",
    "        p.data+= -0.1*p.grad\n",
    "        \n",
    "# print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "754da5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we notive here, t takes a lot if tme for each iteration bcz we are forwarding and backwording >200k examples\n",
    "# in practice we perform Fp and Bp ib minibatches selected randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009bac21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297357fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b0498eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 1, 3, 3, 0, 1, 0, 4, 3, 0, 3, 3, 3, 4, 3, 1, 2, 0, 4, 2, 0,\n",
       "        3, 2, 1, 2, 4, 1, 2, 1])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0,5,(32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "2c684189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 96133, 124371,  74113,  82501, 201194, 210880,  80911, 212742, 220099,\n",
       "        153815,   8834, 204541, 174928,  57400, 150157, 113833, 225204,  76476,\n",
       "         27454, 103451, 118790, 104608,  34161, 102525, 114944, 181805, 199220,\n",
       "        106520,  44028, 166113,  92750,  64817])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0,X.shape[0],(32,))  # this creates int that index into our dataset and theres 32 of them so our minibatch size is 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "29737f3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.933703422546387\n",
      "6.000620365142822\n",
      "8.13911247253418\n",
      "9.000179290771484\n",
      "6.593320369720459\n",
      "9.055007934570312\n",
      "7.657347202301025\n",
      "6.197051048278809\n",
      "7.0187578201293945\n",
      "6.425689220428467\n",
      "7.118587493896484\n",
      "7.458771705627441\n",
      "6.7737321853637695\n",
      "8.155210494995117\n",
      "7.194069862365723\n",
      "8.774280548095703\n",
      "6.537813186645508\n",
      "5.187815189361572\n",
      "4.534985065460205\n",
      "4.850400447845459\n",
      "5.70827579498291\n",
      "6.165180206298828\n",
      "5.682713031768799\n",
      "6.983326435089111\n",
      "5.497468948364258\n",
      "6.571448802947998\n",
      "5.212042331695557\n",
      "4.348809719085693\n",
      "5.1240234375\n",
      "5.448896408081055\n",
      "6.194093227386475\n",
      "4.600247383117676\n",
      "5.39347505569458\n",
      "6.066452503204346\n",
      "4.671658992767334\n",
      "5.766372203826904\n",
      "4.061760425567627\n",
      "3.9099063873291016\n",
      "4.472407817840576\n",
      "5.750327110290527\n",
      "4.34683084487915\n",
      "4.314275741577148\n",
      "4.822473049163818\n",
      "5.943849563598633\n",
      "4.155023097991943\n",
      "4.468928813934326\n",
      "4.472998142242432\n",
      "3.473346710205078\n",
      "5.655151844024658\n",
      "3.2892184257507324\n",
      "4.634355545043945\n",
      "3.7700142860412598\n",
      "4.009340763092041\n",
      "4.110291004180908\n",
      "4.726177215576172\n",
      "4.608971118927002\n",
      "5.490658760070801\n",
      "3.773766279220581\n",
      "3.6023051738739014\n",
      "4.9530348777771\n",
      "4.139883995056152\n",
      "4.740331649780273\n",
      "3.6523027420043945\n",
      "3.8795719146728516\n",
      "3.4009454250335693\n",
      "3.4468510150909424\n",
      "4.323436737060547\n",
      "3.6375808715820312\n",
      "4.064093112945557\n",
      "4.648816108703613\n",
      "4.048853397369385\n",
      "3.5739474296569824\n",
      "3.632535457611084\n",
      "3.384162425994873\n",
      "3.7051665782928467\n",
      "4.028375148773193\n",
      "3.9534077644348145\n",
      "3.2477145195007324\n",
      "3.7470552921295166\n",
      "3.835026979446411\n",
      "3.6823861598968506\n",
      "4.491570472717285\n",
      "4.0316033363342285\n",
      "4.1560540199279785\n",
      "3.976679801940918\n",
      "4.067773818969727\n",
      "3.591977834701538\n",
      "3.5735743045806885\n",
      "4.144206523895264\n",
      "3.8967275619506836\n",
      "3.923900604248047\n",
      "3.4652113914489746\n",
      "3.1599724292755127\n",
      "3.8025970458984375\n",
      "2.7534852027893066\n",
      "3.072758436203003\n",
      "3.657630681991577\n",
      "3.1705305576324463\n",
      "3.905888080596924\n",
      "4.621860980987549\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100): \n",
    "    \n",
    "    #minibatch construct\n",
    "    ix=torch.randint(0,X.shape[0],(32,))\n",
    "    \n",
    "    #forward pass\n",
    "    emb=C[X[ix]] #(32,3,2)\n",
    "    h=torch.tanh(emb.view(-1,6) @ W1 +b1) #32x100\n",
    "    logits=h @ W2 +b2 #32x27\n",
    "    loss=F.cross_entropy(logits,Y[ix])\n",
    "    print(loss.item())\n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad=None            #set gradients to 0\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    #upgarde\n",
    "    for p in parameters:\n",
    "        p.data+= -0.1*p.grad\n",
    "        \n",
    "# print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2837e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#minibatch:decrease the loss much much faster but \n",
    "#quality of gradient not lower thus direction is not reliable:not actual gradient direction\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "4dd8efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#continue doing optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "dff7578a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.920896291732788\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100): \n",
    "    \n",
    "    #minibatch construct\n",
    "    ix=torch.randint(0,X.shape[0],(32,))\n",
    "    \n",
    "    #forward pass\n",
    "    emb=C[X[ix]] #(32,3,2)\n",
    "    h=torch.tanh(emb.view(-1,6) @ W1 +b1) #32x100\n",
    "    logits=h @ W2 +b2 #32x27\n",
    "    loss=F.cross_entropy(logits,Y[ix])\n",
    "    \n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad=None            #set gradients to 0\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    #upgarde\n",
    "    for p in parameters:\n",
    "        p.data+= -0.1*p.grad\n",
    "        \n",
    "print(loss.item())\n",
    "# this is loss for this particular minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "e13f7f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8090016841888428"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eval loss for All of X and Y\n",
    "\n",
    "emb=C[X] #(32,3,2)\n",
    "h=torch.tanh(emb.view(-1,6) @ W1 +b1) #32x100\n",
    "logits=h @ W2 +b2 #32x27\n",
    "loss=F.cross_entropy(logits,Y)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "91d37d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so now r at 2.8, lets run optimization for a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "7af2a793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4167394638061523\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000): \n",
    "    \n",
    "    #minibatch construct\n",
    "    ix=torch.randint(0,X.shape[0],(32,))\n",
    "    \n",
    "    #forward pass\n",
    "    emb=C[X[ix]] #(32,3,2)\n",
    "    h=torch.tanh(emb.view(-1,6) @ W1 +b1) #32x100\n",
    "    logits=h @ W2 +b2 #32x27\n",
    "    loss=F.cross_entropy(logits,Y[ix])\n",
    "    \n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad=None            #set gradients to 0\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    #upgarde\n",
    "    for p in parameters:\n",
    "        p.data+= -0.1*p.grad\n",
    "        \n",
    "print(loss.item())\n",
    "# this is loss for this particular minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "de2578e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.570652723312378"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eval loss for All of X and Y\n",
    "\n",
    "emb=C[X] #(32,3,2)\n",
    "h=torch.tanh(emb.view(-1,6) @ W1 +b1) #32x100\n",
    "logits=h @ W2 +b2 #32x27\n",
    "loss=F.cross_entropy(logits,Y)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2977285a",
   "metadata": {},
   "source": [
    "# Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "2b29ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to determine learning rate what is the right speed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "9f228b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets reset our parameters to initial\n",
    "#try lr for 0.0001,0.001: very slow, 1,: unstable ,10: not optimizing, right lr between 0.001 and -1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "d1ba9b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080, 0.0090,\n",
       "        0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170, 0.0180,\n",
       "        0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260, 0.0270,\n",
       "        0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350, 0.0360,\n",
       "        0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440, 0.0450,\n",
       "        0.0460, 0.0470, 0.0480, 0.0490, 0.0500, 0.0510, 0.0520, 0.0530, 0.0540,\n",
       "        0.0550, 0.0560, 0.0570, 0.0580, 0.0590, 0.0600, 0.0610, 0.0620, 0.0630,\n",
       "        0.0640, 0.0650, 0.0660, 0.0670, 0.0680, 0.0690, 0.0700, 0.0710, 0.0720,\n",
       "        0.0730, 0.0740, 0.0750, 0.0760, 0.0770, 0.0780, 0.0790, 0.0800, 0.0810,\n",
       "        0.0820, 0.0830, 0.0840, 0.0850, 0.0860, 0.0870, 0.0880, 0.0890, 0.0900,\n",
       "        0.0910, 0.0920, 0.0930, 0.0940, 0.0950, 0.0960, 0.0970, 0.0980, 0.0990,\n",
       "        0.1000, 0.1010, 0.1020, 0.1030, 0.1040, 0.1050, 0.1060, 0.1070, 0.1080,\n",
       "        0.1090, 0.1100, 0.1110, 0.1120, 0.1130, 0.1140, 0.1150, 0.1160, 0.1170,\n",
       "        0.1180, 0.1190, 0.1200, 0.1210, 0.1220, 0.1230, 0.1240, 0.1250, 0.1260,\n",
       "        0.1270, 0.1280, 0.1290, 0.1300, 0.1310, 0.1320, 0.1330, 0.1340, 0.1350,\n",
       "        0.1360, 0.1370, 0.1380, 0.1390, 0.1400, 0.1410, 0.1420, 0.1430, 0.1440,\n",
       "        0.1450, 0.1460, 0.1470, 0.1480, 0.1490, 0.1500, 0.1510, 0.1520, 0.1530,\n",
       "        0.1540, 0.1550, 0.1560, 0.1570, 0.1580, 0.1590, 0.1600, 0.1610, 0.1620,\n",
       "        0.1630, 0.1640, 0.1650, 0.1660, 0.1670, 0.1680, 0.1690, 0.1700, 0.1710,\n",
       "        0.1720, 0.1730, 0.1740, 0.1750, 0.1760, 0.1770, 0.1780, 0.1790, 0.1800,\n",
       "        0.1810, 0.1820, 0.1830, 0.1840, 0.1850, 0.1860, 0.1870, 0.1880, 0.1890,\n",
       "        0.1900, 0.1910, 0.1920, 0.1930, 0.1940, 0.1950, 0.1960, 0.1970, 0.1980,\n",
       "        0.1990, 0.2000, 0.2010, 0.2020, 0.2030, 0.2040, 0.2050, 0.2060, 0.2070,\n",
       "        0.2080, 0.2090, 0.2100, 0.2110, 0.2120, 0.2130, 0.2140, 0.2150, 0.2160,\n",
       "        0.2170, 0.2180, 0.2190, 0.2200, 0.2210, 0.2220, 0.2230, 0.2240, 0.2250,\n",
       "        0.2260, 0.2270, 0.2280, 0.2290, 0.2300, 0.2310, 0.2320, 0.2330, 0.2340,\n",
       "        0.2350, 0.2360, 0.2370, 0.2380, 0.2390, 0.2400, 0.2410, 0.2420, 0.2430,\n",
       "        0.2440, 0.2450, 0.2460, 0.2470, 0.2480, 0.2490, 0.2500, 0.2510, 0.2520,\n",
       "        0.2530, 0.2540, 0.2550, 0.2560, 0.2570, 0.2580, 0.2590, 0.2600, 0.2610,\n",
       "        0.2620, 0.2630, 0.2640, 0.2650, 0.2660, 0.2670, 0.2680, 0.2690, 0.2700,\n",
       "        0.2710, 0.2720, 0.2730, 0.2740, 0.2750, 0.2760, 0.2770, 0.2780, 0.2790,\n",
       "        0.2800, 0.2810, 0.2820, 0.2830, 0.2840, 0.2850, 0.2860, 0.2870, 0.2880,\n",
       "        0.2890, 0.2900, 0.2910, 0.2920, 0.2930, 0.2940, 0.2950, 0.2960, 0.2970,\n",
       "        0.2980, 0.2990, 0.3000, 0.3010, 0.3020, 0.3030, 0.3040, 0.3050, 0.3060,\n",
       "        0.3070, 0.3080, 0.3090, 0.3100, 0.3110, 0.3120, 0.3130, 0.3140, 0.3150,\n",
       "        0.3160, 0.3170, 0.3180, 0.3190, 0.3200, 0.3210, 0.3220, 0.3230, 0.3240,\n",
       "        0.3250, 0.3260, 0.3270, 0.3280, 0.3290, 0.3300, 0.3310, 0.3320, 0.3330,\n",
       "        0.3340, 0.3350, 0.3360, 0.3370, 0.3380, 0.3390, 0.3400, 0.3410, 0.3420,\n",
       "        0.3430, 0.3440, 0.3450, 0.3460, 0.3470, 0.3480, 0.3490, 0.3500, 0.3510,\n",
       "        0.3520, 0.3530, 0.3540, 0.3550, 0.3560, 0.3570, 0.3580, 0.3590, 0.3600,\n",
       "        0.3610, 0.3620, 0.3630, 0.3640, 0.3650, 0.3660, 0.3670, 0.3680, 0.3690,\n",
       "        0.3700, 0.3710, 0.3720, 0.3730, 0.3740, 0.3750, 0.3760, 0.3770, 0.3780,\n",
       "        0.3790, 0.3800, 0.3810, 0.3820, 0.3830, 0.3840, 0.3850, 0.3860, 0.3870,\n",
       "        0.3880, 0.3890, 0.3900, 0.3910, 0.3920, 0.3930, 0.3940, 0.3950, 0.3960,\n",
       "        0.3970, 0.3980, 0.3990, 0.4000, 0.4010, 0.4020, 0.4030, 0.4040, 0.4050,\n",
       "        0.4060, 0.4070, 0.4080, 0.4090, 0.4100, 0.4110, 0.4120, 0.4130, 0.4140,\n",
       "        0.4150, 0.4160, 0.4170, 0.4180, 0.4190, 0.4200, 0.4210, 0.4220, 0.4230,\n",
       "        0.4240, 0.4250, 0.4260, 0.4270, 0.4280, 0.4290, 0.4300, 0.4310, 0.4320,\n",
       "        0.4330, 0.4340, 0.4350, 0.4360, 0.4370, 0.4380, 0.4390, 0.4400, 0.4410,\n",
       "        0.4420, 0.4430, 0.4440, 0.4450, 0.4460, 0.4470, 0.4480, 0.4490, 0.4500,\n",
       "        0.4510, 0.4520, 0.4530, 0.4540, 0.4550, 0.4560, 0.4570, 0.4580, 0.4590,\n",
       "        0.4600, 0.4610, 0.4620, 0.4630, 0.4640, 0.4650, 0.4660, 0.4670, 0.4680,\n",
       "        0.4690, 0.4700, 0.4710, 0.4720, 0.4730, 0.4740, 0.4750, 0.4760, 0.4770,\n",
       "        0.4780, 0.4790, 0.4800, 0.4810, 0.4820, 0.4830, 0.4840, 0.4850, 0.4860,\n",
       "        0.4870, 0.4880, 0.4890, 0.4900, 0.4910, 0.4920, 0.4930, 0.4940, 0.4950,\n",
       "        0.4960, 0.4970, 0.4980, 0.4990, 0.5000, 0.5010, 0.5020, 0.5030, 0.5040,\n",
       "        0.5050, 0.5060, 0.5070, 0.5080, 0.5090, 0.5100, 0.5110, 0.5120, 0.5130,\n",
       "        0.5140, 0.5150, 0.5160, 0.5170, 0.5180, 0.5190, 0.5200, 0.5210, 0.5220,\n",
       "        0.5230, 0.5240, 0.5250, 0.5260, 0.5270, 0.5280, 0.5290, 0.5300, 0.5310,\n",
       "        0.5320, 0.5330, 0.5340, 0.5350, 0.5360, 0.5370, 0.5380, 0.5390, 0.5400,\n",
       "        0.5410, 0.5420, 0.5430, 0.5440, 0.5450, 0.5460, 0.5470, 0.5480, 0.5490,\n",
       "        0.5500, 0.5510, 0.5520, 0.5530, 0.5540, 0.5550, 0.5560, 0.5570, 0.5580,\n",
       "        0.5590, 0.5600, 0.5610, 0.5620, 0.5630, 0.5640, 0.5650, 0.5660, 0.5670,\n",
       "        0.5680, 0.5690, 0.5700, 0.5710, 0.5720, 0.5730, 0.5740, 0.5750, 0.5760,\n",
       "        0.5770, 0.5780, 0.5790, 0.5800, 0.5810, 0.5820, 0.5830, 0.5840, 0.5850,\n",
       "        0.5860, 0.5870, 0.5880, 0.5890, 0.5900, 0.5910, 0.5920, 0.5930, 0.5940,\n",
       "        0.5950, 0.5960, 0.5970, 0.5980, 0.5990, 0.6000, 0.6010, 0.6020, 0.6030,\n",
       "        0.6040, 0.6050, 0.6060, 0.6070, 0.6080, 0.6090, 0.6100, 0.6110, 0.6120,\n",
       "        0.6130, 0.6140, 0.6150, 0.6160, 0.6170, 0.6180, 0.6190, 0.6200, 0.6210,\n",
       "        0.6220, 0.6230, 0.6240, 0.6250, 0.6260, 0.6270, 0.6280, 0.6290, 0.6300,\n",
       "        0.6310, 0.6320, 0.6330, 0.6340, 0.6350, 0.6360, 0.6370, 0.6380, 0.6390,\n",
       "        0.6400, 0.6410, 0.6420, 0.6430, 0.6440, 0.6450, 0.6460, 0.6470, 0.6480,\n",
       "        0.6490, 0.6500, 0.6510, 0.6520, 0.6530, 0.6540, 0.6550, 0.6560, 0.6570,\n",
       "        0.6580, 0.6590, 0.6600, 0.6610, 0.6620, 0.6630, 0.6640, 0.6650, 0.6660,\n",
       "        0.6670, 0.6680, 0.6690, 0.6700, 0.6710, 0.6720, 0.6730, 0.6740, 0.6750,\n",
       "        0.6760, 0.6770, 0.6780, 0.6790, 0.6800, 0.6810, 0.6820, 0.6830, 0.6840,\n",
       "        0.6850, 0.6860, 0.6870, 0.6880, 0.6890, 0.6900, 0.6910, 0.6920, 0.6930,\n",
       "        0.6940, 0.6950, 0.6960, 0.6970, 0.6980, 0.6990, 0.7000, 0.7010, 0.7020,\n",
       "        0.7030, 0.7040, 0.7050, 0.7060, 0.7070, 0.7080, 0.7090, 0.7100, 0.7110,\n",
       "        0.7120, 0.7130, 0.7140, 0.7150, 0.7160, 0.7170, 0.7180, 0.7190, 0.7200,\n",
       "        0.7210, 0.7220, 0.7230, 0.7240, 0.7250, 0.7260, 0.7270, 0.7280, 0.7290,\n",
       "        0.7300, 0.7310, 0.7320, 0.7330, 0.7340, 0.7350, 0.7360, 0.7370, 0.7380,\n",
       "        0.7390, 0.7400, 0.7410, 0.7420, 0.7430, 0.7440, 0.7450, 0.7460, 0.7470,\n",
       "        0.7480, 0.7490, 0.7500, 0.7510, 0.7520, 0.7530, 0.7540, 0.7550, 0.7560,\n",
       "        0.7570, 0.7580, 0.7590, 0.7600, 0.7610, 0.7620, 0.7630, 0.7640, 0.7650,\n",
       "        0.7660, 0.7670, 0.7680, 0.7690, 0.7700, 0.7710, 0.7720, 0.7730, 0.7740,\n",
       "        0.7750, 0.7760, 0.7770, 0.7780, 0.7790, 0.7800, 0.7810, 0.7820, 0.7830,\n",
       "        0.7840, 0.7850, 0.7860, 0.7870, 0.7880, 0.7890, 0.7900, 0.7910, 0.7920,\n",
       "        0.7930, 0.7940, 0.7950, 0.7960, 0.7970, 0.7980, 0.7990, 0.8000, 0.8010,\n",
       "        0.8020, 0.8030, 0.8040, 0.8050, 0.8060, 0.8070, 0.8080, 0.8090, 0.8100,\n",
       "        0.8110, 0.8120, 0.8130, 0.8140, 0.8150, 0.8160, 0.8170, 0.8180, 0.8190,\n",
       "        0.8200, 0.8210, 0.8220, 0.8230, 0.8240, 0.8250, 0.8260, 0.8270, 0.8280,\n",
       "        0.8290, 0.8300, 0.8310, 0.8320, 0.8330, 0.8340, 0.8350, 0.8360, 0.8370,\n",
       "        0.8380, 0.8390, 0.8400, 0.8410, 0.8420, 0.8430, 0.8440, 0.8450, 0.8460,\n",
       "        0.8470, 0.8480, 0.8490, 0.8500, 0.8510, 0.8520, 0.8530, 0.8540, 0.8550,\n",
       "        0.8560, 0.8570, 0.8580, 0.8590, 0.8600, 0.8610, 0.8620, 0.8630, 0.8640,\n",
       "        0.8650, 0.8660, 0.8670, 0.8680, 0.8690, 0.8700, 0.8710, 0.8720, 0.8730,\n",
       "        0.8740, 0.8750, 0.8760, 0.8770, 0.8780, 0.8790, 0.8800, 0.8810, 0.8820,\n",
       "        0.8830, 0.8840, 0.8850, 0.8860, 0.8870, 0.8880, 0.8890, 0.8900, 0.8910,\n",
       "        0.8920, 0.8930, 0.8940, 0.8950, 0.8960, 0.8970, 0.8980, 0.8990, 0.9000,\n",
       "        0.9010, 0.9020, 0.9030, 0.9040, 0.9050, 0.9060, 0.9070, 0.9080, 0.9090,\n",
       "        0.9100, 0.9110, 0.9120, 0.9130, 0.9140, 0.9150, 0.9160, 0.9170, 0.9180,\n",
       "        0.9190, 0.9200, 0.9210, 0.9220, 0.9230, 0.9240, 0.9250, 0.9260, 0.9270,\n",
       "        0.9280, 0.9290, 0.9300, 0.9310, 0.9320, 0.9330, 0.9340, 0.9350, 0.9360,\n",
       "        0.9370, 0.9380, 0.9390, 0.9400, 0.9410, 0.9420, 0.9430, 0.9440, 0.9450,\n",
       "        0.9460, 0.9470, 0.9480, 0.9490, 0.9500, 0.9510, 0.9520, 0.9530, 0.9540,\n",
       "        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,\n",
       "        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,\n",
       "        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,\n",
       "        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,\n",
       "        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(0.001,1,1000)#doesnt make sense to change linearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "c51590db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lre=torch.linspace(-3,0,1000)\n",
    "lrs=10**lre\n",
    "lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "bb36d925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.61003589630127\n"
     ]
    }
   ],
   "source": [
    "lri=[]\n",
    "lossi=[]\n",
    "\n",
    "\n",
    "for i in range(1000): \n",
    "    \n",
    "    #minibatch construct\n",
    "    ix=torch.randint(0,X.shape[0],(32,))\n",
    "    \n",
    "    #forward pass\n",
    "    emb=C[X[ix]] #(32,3,2)\n",
    "    h=torch.tanh(emb.view(-1,6) @ W1 +b1) #32x100\n",
    "    logits=h @ W2 +b2 #32x27\n",
    "    loss=F.cross_entropy(logits,Y[ix])\n",
    "    \n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad=None            #set gradients to 0\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    #upgarde \n",
    "    lr=lrs[i]              # we start with very less lr and go go upto -1\n",
    "    for p in parameters:\n",
    "        p.data+= -lr*p.grad \n",
    "        \n",
    "        #track stats\n",
    "        lri.append(lre[i])\n",
    "        lossi.append(loss.item())\n",
    "        \n",
    "        \n",
    "print(loss.item())\n",
    "# this is loss for this particular minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "b25d2146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e3884bf250>]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plots\n",
    "plt.plot(lri,lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "6fd451af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7639577388763428\n"
     ]
    }
   ],
   "source": [
    "#finalising LR\n",
    "#reset parameters\n",
    "\n",
    "lri=[]\n",
    "lossi=[]\n",
    "\n",
    "\n",
    "for i in range(10000): \n",
    "    \n",
    "    #minibatch construct\n",
    "    ix=torch.randint(0,X.shape[0],(32,))\n",
    "    \n",
    "    #forward pass\n",
    "    emb=C[X[ix]] #(32,3,2)\n",
    "    h=torch.tanh(emb.view(-1,6) @ W1 +b1) #32x100\n",
    "    logits=h @ W2 +b2 #32x27\n",
    "    loss=F.cross_entropy(logits,Y[ix])\n",
    "    \n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad=None            #set gradients to 0\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    #upgarde \n",
    "#     lr=lrs[i]              # we start with very less lr and go go upto -1\n",
    "    lr=0.01\n",
    "    for p in parameters:\n",
    "        p.data+= -lr*p.grad \n",
    "        \n",
    "        #track stats\n",
    "#         lri.append(lre[i])\n",
    "#         lossi.append(loss.item())\n",
    "        \n",
    "        \n",
    "print(loss.item())\n",
    "# this is loss for this particular minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "79e0a479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4635331630706787"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eval loss for All of X and Y\n",
    "\n",
    "emb=C[X] #(32,3,2)\n",
    "h=torch.tanh(emb.view(-1,6) @ W1 +b1) #32x100\n",
    "logits=h @ W2 +b2 #32x27\n",
    "loss=F.cross_entropy(logits,Y)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "59d465fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning Rate decay: late stage of training we want to go slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "301d2bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.435790777206421\n"
     ]
    }
   ],
   "source": [
    "#finalising LR\n",
    "#reset parameters\n",
    "\n",
    "lri=[]\n",
    "lossi=[]\n",
    "\n",
    "\n",
    "for i in range(10000): \n",
    "    \n",
    "    #minibatch construct\n",
    "    ix=torch.randint(0,X.shape[0],(32,))\n",
    "    \n",
    "    #forward pass\n",
    "    emb=C[X[ix]] #(32,3,2)\n",
    "    h=torch.tanh(emb.view(-1,6) @ W1 +b1) #32x100\n",
    "    logits=h @ W2 +b2 #32x27\n",
    "    loss=F.cross_entropy(logits,Y[ix])\n",
    "    \n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad=None            #set gradients to 0\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    #upgarde \n",
    "#     lr=lrs[i]              # we start with very less lr and go go upto -1\n",
    "    lr=0.001\n",
    "    for p in parameters:\n",
    "        p.data+= -lr*p.grad \n",
    "        \n",
    "        #track stats\n",
    "#         lri.append(lre[i])\n",
    "#         lossi.append(loss.item())\n",
    "        \n",
    "        \n",
    "print(loss.item())\n",
    "# this is loss for this particular minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "59e93019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4495561122894287"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eval loss for All of X and Y\n",
    "\n",
    "emb=C[X] #(32,3,2)\n",
    "h=torch.tanh(emb.view(-1,6) @ W1 +b1) #32x100\n",
    "logits=h @ W2 +b2 #32x27\n",
    "loss=F.cross_entropy(logits,Y)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "cdb6168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#caution: better model bcz loss < bigram model: not true bcz this is small model: larger if add parametr and neurons\n",
    "# as capacity of NN expands, it becomes more and capable of overfitting, loss goes to 0 but model is learning the \n",
    "# input rather than generalizing so out is same nothing new thus will perform bad on unseen data, the loss will be very high\n",
    "# so split up the data: tain, val/dev, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1685d4",
   "metadata": {},
   "source": [
    "see part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d2d94f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
